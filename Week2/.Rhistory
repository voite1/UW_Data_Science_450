install.packages("plyr")
install.packages("ggplot2")
g = rnorm(10,5)
plot(g)
pwd()
getwd()
install.packages("UsingR")
ls -l
exit()
install.packages(c("boot", "class", "cluster", "codetools", "colorspace", "evaluate", "foreign", "formatR", "Formula", "ggplot2", "highr", "Hmisc", "jsonlite", "KernSmooth", "knitr", "lattice", "manipulate", "markdown", "MASS", "Matrix", "mgcv", "mime", "nlme", "nnet", "plyr", "Rcpp", "RCurl", "rpart", "spatial", "stringr", "survival"))
library(data.table)
library(logging)
library(RSQLite)
install.packages("data.table", "logging", "RSQLite")
install.packages("data.table")
install.packages('logging')
install.packages(RSQLite)
install.packages("RSQLite")
library(data.table)
library(logging)
library(RSQLite)
list.dirs
A_matrix = matrix(4, nrow=4, ncol=3) # Makes use of broadcasting
View(A_matrix)
B_matrix = matrix(1:50, nrow=4, ncol=3)
View(B_matrix)
A_matrix + B_matrix
A_matrix * B_matrix
A_matrix %*% B_matrix
A_matrix %*% t(B_matrix)
x_values = seq(from=as.Date('2015-01-01'),
to=as.Date('2015-02-12'),
by = 3)
df = data.frame('dates' = x_values,
'x1'    = runif(15,-10,20),
'x2'    = 1:15,
'x3'    = strsplit('MississippiMath','')[[1]])
View(df)
df$x3 = as.character(df$x3)
df$x3 = tolower(df$x3)
View(df)
str(df)
head(df)
tail(df, n=10)
df = as.data.table(df)
View(df)
df[,sum(x1)]
df[,c(sum(x1),sd(x2))]
?logging
x_values
install.packages("XML")
nfl_site = "http://www.usatoday.com/sports/nfl/arrests/"
nfl_html = readHTMLTable(nfl_site)
nfl_site = "http://www.usatoday.com/sports/nfl/arrests/"
library(XML)
nfl_html = readHTMLTable(nfl_site)
nfl_html = readHTMLTable(nfl_site)
nfl_html
nfl_data = nfl_html[[1]]
nfl_html
db_file = 'test_db.db'
conn = dbConnect(dbDriver("SQLite"), dbname=db_file)
dbWriteTable(conn, 'table_name', medals_data, overwrite=TRUE)
medals_data <- read.table("medals.txt", sep="\t", header=TRUE)
p = 0.75
n = 1000
?rbinom
bern_samples = rbinom(n, 1, p)
bern_sample_mean = sum(bern_samples)/length(bern_samples)
bern_sample_var = bern_sample_mean * (1-bern_sample_mean)
##--------------------------------------------
##
## Exploring Data in R (lecture 1)
##
## Class: PCE Data Science Methods Class
##
## Contains examples of:
##
## -Working with Distributions
##
## -Visually/Numerically Exploring data
##
##--------------------------------------------
##-----Exploring data Visually----
# Bernoulli (Binomial with n = 1)
p = 0.75
n = 1000
bern_samples = rbinom(n, 1, p)
bern_sample_mean = sum(bern_samples)/length(bern_samples)
bern_sample_var = bern_sample_mean * (1-bern_sample_mean)
bern_var = p*(1-p)
# Binomial
N = c(5, 25, 75)
binom_samples = lapply(N, function(x) rbinom(n, x, p))
binom_sample_means = lapply(binom_samples, mean)
binom_means = N*p
binom_sample_vars = lapply(binom_samples, var)
binom_vars = N*p*(1-p)
par(mfrow=c(2,2))
for (i in 1:3){
hist(binom_samples[[i]], main=paste(N[i],'Experiments'), freq=FALSE)
x_norm = seq(0,N[i], by = 0.025)
y_norm = dnorm(x_norm, mean=binom_means[i], sd=sqrt(binom_vars[i]))
lines(x_norm, y_norm)
}
##-------------------------------------------------
## Aleksey Kramer
## Data Science 350
## Final Project
## Analysis of Seattle Restaurant Inspection Data
##-------------------------------------------------
require(logging)
# Set working directory
#setwd('C:\\Users\\Aleksey\\Documents\\School\\UW_Data_Science\\UW_Data_Science_350\\FinalTWITProject')
setwd('/Users/voitel/TRAINING/UW_Data_Science/UW_Data_Science_350/FinalTWITProject')
# Filename to save
f_name <- "restaurant.csv"
if(!file.exists(f_name)) {
# URL to download data from
url <- "https://data.kingcounty.gov/api/views/pph9-v8tz/rows.csv?accessType=DOWNLOAD"
# Download and save data
# FOR WINDOWS REMOVE: method = "curl"
download.file(url,destfile = f_name, method = "curl")
}
# Read data file
r_data <- read.csv(f_name, stringsAsFactors = FALSE, na.strings = "")
# Look at the names
names(r_data)
# Removing Phone data because it is irrelevant and incomplete
r_data$Phone <- NULL
# Isolate Seattle only
r_data <- r_data[r_data$City == "Seattle",]
# Remove City filed (not needed any more)
r_data$City <- NULL
# Check the dimension of the data
dim(r_data)
# Replace all N/A valuses in with "none" to save the records
r_data$Violation.Type[is.na(r_data$Violation.Type)] <- "none"
r_data$Violation.Description[is.na(r_data$Violation.Description)] <- "none"
r_data$Violation_Record_ID[is.na(r_data$Violation_Record_ID)] <- "none"
# Convert date field to Date Type
r_data$Inspection.Date <- as.Date(r_data$Inspection.Date, format = "%m/%d/%Y")
# Isolate year 2015 only (if needed)
# r_data <- r_data[r_data$Inspection.Date > '2014-12-31',]
# Try histograms on violation points (not good)
hist(r_data$Violation.Points)
# Try log of violation points (much better)
hist(log(r_data$Violation.Points))
##-------------------------------------------------
## Aleksey Kramer
## Data Science 350
## Final Project
## Analysis of Seattle Restaurant Inspection Data
##-------------------------------------------------
require(logging)
# Set working directory
#setwd('C:\\Users\\Aleksey\\Documents\\School\\UW_Data_Science\\UW_Data_Science_350\\FinalTWITProject')
setwd('/Users/voitel/TRAINING/UW_Data_Science/UW_Data_Science_350/FinalTWITProject')
# Filename to save
f_name <- "restaurant.csv"
if(!file.exists(f_name)) {
# URL to download data from
url <- "https://data.kingcounty.gov/api/views/pph9-v8tz/rows.csv?accessType=DOWNLOAD"
# Download and save data
# FOR WINDOWS REMOVE: method = "curl"
download.file(url,destfile = f_name, method = "curl")
}
# Read data file
r_data <- read.csv(f_name, stringsAsFactors = FALSE, na.strings = "")
# Look at the names
names(r_data)
# Removing Phone data because it is irrelevant and incomplete
r_data$Phone <- NULL
# Isolate Seattle only
r_data <- r_data[r_data$City == "Seattle",]
# Remove City filed (not needed any more)
r_data$City <- NULL
# Check the dimension of the data
dim(r_data)
# Replace all N/A valuses in with "none" to save the records
r_data$Violation.Type[is.na(r_data$Violation.Type)] <- "none"
r_data$Violation.Description[is.na(r_data$Violation.Description)] <- "none"
r_data$Violation_Record_ID[is.na(r_data$Violation_Record_ID)] <- "none"
# Convert date field to Date Type
r_data$Inspection.Date <- as.Date(r_data$Inspection.Date, format = "%m/%d/%Y")
# Isolate year 2015 only (if needed)
# r_data <- r_data[r_data$Inspection.Date > '2014-12-31',]
# cleanup
rm(f_name)
rm(url)
# Try histograms on violation points (not good)
hist(r_data$Violation.Points)
# Try log of violation points (much better)
hist(log(r_data$Violation.Points))
View(r_data)
View(r_data)
unique(r_data$Inspection.Type)
r_data[r_data$Inspection.Type == 'Consultation/Education - Field']
r_data[r_data$Inspection.Type == 'Consultation/Education - Field',]
r_data[r_data$Inspection.Type == 'Consultation/Education - Field',]$my_inspection.type <- 1
r_data[r_data$Inspection.Type == 'Consultation/Education - Field',] <- 1
r_data$my_InspectionType[r_data$Inspection.Type == 'Consultation/Education - Field',] <- 1
r_data$my_InspectionType[r_data$Inspection.Type == 'Consultation/Education - Field'] <- 1
unique(r_data$Inspection.Type)
r_data$my_InspectionType[r_data$Inspection.Type == 'Return Inspection'] <- 3
r_data$my_InspectionType[r_data$Inspection.Type == 'Routine Inspection/Field Review'] <- 2
unique(r_data$Violation.Type)
r_data$my_ViolationType[r_data$Violation.Type == 'none'] <- 1
r_data$my_ViolationType[r_data$Violation.Type == 'blue'] <- 2
r_data$my_ViolationType[r_data$Violation.Type == 'red'] <- 3
distinct(r_data$Inspection.Score)
unique(r_data$Inspection.Score)
unique(r_data$Violation.Points)
hist(r_data$Violation.Points)
unique(r_data$Longitude)
unique(r_data$Latitude)
hist(r_data$Violation.Points)
hist(r_data$Violation.Description)
hist(r_data$Inspection.Score)
hist(log(r_data$Inspection.Score))
save.image("~/TRAINING/UW_Data_Science/UW_Data_Science_350/Final_Project/Untitled.RData")
##-------------------------------------------------
## Aleksey Kramer
## Data Science 350
## Final Project
## Analysis of Seattle Restaurant Inspection Data
##-------------------------------------------------
require(logging)
# Set working directory
#setwd('C:\\Users\\Aleksey\\Documents\\School\\UW_Data_Science\\UW_Data_Science_350\\FinalTWITProject')
setwd('/Users/voitel/TRAINING/UW_Data_Science/UW_Data_Science_350/FinalTWITProject')
# Filename to save
f_name <- "restaurant.csv"
if(!file.exists(f_name)) {
# URL to download data from
url <- "https://data.kingcounty.gov/api/views/pph9-v8tz/rows.csv?accessType=DOWNLOAD"
# Download and save data
# FOR WINDOWS REMOVE: method = "curl"
download.file(url,destfile = f_name, method = "curl")
}
# Read data file
r_data <- read.csv(f_name, stringsAsFactors = FALSE, na.strings = "")
# Look at the names
names(r_data)
# Removing Phone data because it is irrelevant and incomplete
r_data$Phone <- NULL
# Isolate Seattle only
r_data <- r_data[r_data$City == "Seattle",]
# Remove City filed (not needed any more)
r_data$City <- NULL
# Check the dimension of the data
dim(r_data)
# Replace all N/A valuses in with "none" to save the records
r_data$Violation.Type[is.na(r_data$Violation.Type)] <- "none"
r_data$Violation.Description[is.na(r_data$Violation.Description)] <- "none"
r_data$Violation_Record_ID[is.na(r_data$Violation_Record_ID)] <- "none"
# Convert date field to Date Type
r_data$Inspection.Date <- as.Date(r_data$Inspection.Date, format = "%m/%d/%Y")
# cleanup unused variables
rm(f_name)
rm(url)
# Add numeric equivalelnt of Inspection Type column to r_data data frame
unique(r_data$Inspection.Type)
r_data$my_InspectionType[r_data$Inspection.Type == 'Consultation/Education - Field'] <- 1
r_data$my_InspectionType[r_data$Inspection.Type == 'Routine Inspection/Field Review'] <- 2
r_data$my_InspectionType[r_data$Inspection.Type == 'Return Inspection'] <- 3
# Add numeric equivalient of Violation Type ro r_data data frame
unique(r_data$Violation.Type)
r_data$my_ViolationType[r_data$Violation.Type == 'none'] <- 1
r_data$my_ViolationType[r_data$Violation.Type == 'blue'] <- 2
r_data$my_ViolationType[r_data$Violation.Type == 'red'] <- 3
# Try using Longtitude, Lattitude, log(inspection score), and log(violation points) to run statistics
unique(r_data$Longitude)
length(unique(r_data$Longitude))
lendth(r_data$Longitude)
length(r_data$Longitude)
str(r_data)
hist(r_data$Longitude)
hist(r_data$Longitude)
require(logging)
require(nortest)
install.packages("nortest")
install.packages("nortest")
require(nortest)
# Set working directory
setwd('C:\\Users\\Aleksey\\Documents\\School\\UW_Data_Science\\UW_Data_Science_350\\FinalTWITProject')
# setwd('/Users/voitel/TRAINING/UW_Data_Science/UW_Data_Science_350/FinalTWITProject')
# Filename to save
f_name <- "restaurant.csv"
if(!file.exists(f_name)) {
# URL to download data from
url <- "https://data.kingcounty.gov/api/views/pph9-v8tz/rows.csv?accessType=DOWNLOAD"
# Download and save data
# For Mac
# download.file(url,destfile = f_name, method = "curl")
# For Windows
download.file(url,destfile = f_name)
}
# Read data file
r_data <- read.csv(f_name, stringsAsFactors = FALSE, na.strings = "")
# Look at the names
names(r_data)
# Removing Phone data because it is irrelevant and incomplete
r_data$Phone <- NULL
# Isolate Seattle only
r_data <- r_data[r_data$City == "Seattle",]
# Remove City filed (not needed any more)
r_data$City <- NULL
# Check the dimension of the data
dim(r_data)
# Replace all N/A valuses in with "none" to save the records
r_data$Violation.Type[is.na(r_data$Violation.Type)] <- "none"
r_data$Violation.Description[is.na(r_data$Violation.Description)] <- "none"
r_data$Violation_Record_ID[is.na(r_data$Violation_Record_ID)] <- "none"
# Convert date field to Date Type
r_data$Inspection.Date <- as.Date(r_data$Inspection.Date, format = "%m/%d/%Y")
# cleanup unused variables
rm(f_name)
rm(url)
# Add numeric equivalelnt of Inspection Type column to r_data data frame
unique(r_data$Inspection.Type)
r_data$my_InspectionType[r_data$Inspection.Type == 'Consultation/Education - Field'] <- 1
r_data$my_InspectionType[r_data$Inspection.Type == 'Routine Inspection/Field Review'] <- 2
r_data$my_InspectionType[r_data$Inspection.Type == 'Return Inspection'] <- 3
# Add numeric equivalient of Violation Type ro r_data data frame
unique(r_data$Violation.Type)
r_data$my_ViolationType[r_data$Violation.Type == 'none'] <- 1
r_data$my_ViolationType[r_data$Violation.Type == 'blue'] <- 2
r_data$my_ViolationType[r_data$Violation.Type == 'red'] <- 3
# Check Longtitude and Lattitude variables for Normality
var_longt <- ad.test(r_data$Longitude)
var_latt <- ad.test(r_data$Latitude)
# Output p-value for Longtitute and Lattitude variables (both of them
# are normally distributed).  Paremetric tests should be used.
print(paste('p-value for normality test of Longtitude variable is:', var_longt$p.value))
print(paste('p-value for normality test of Lattitude variable is:', var_latt$p.value))
# Try using Longtitude, Lattitude to run statistics against my_inspectionType and my_ViolationType
url <- "https://data.kingcounty.gov/api/views/pph9-v8tz/rows.csv?accessType=DOWNLOAD"
rm(url)
rm(var_latt)
rm(var_longt)
install.packages(c("prodlim", "RcppArmadillo", "splancs"))
princomp()
aris
iris
princomp(iris[,-5])
fit <- princomp(iris[,-5])
summary(fit)
fit$sdev
fit$sdev[,1]
fit$sdev[,"Comp.1"]
fit$sdev["Comp.1"]
fit$sdev[1]
fit$sdev[1]$name
str(fit$sdev[1])
setwd('/Users/voitel/TRAINING/UW_Data_Science/UW_Data_Science_450/Week2')
file <- '   /Users/voitel/TRAINING/UW_Data_Science/UW_Data_Science_450/Week2/monthly_avg_gas_usage_jan71_oct79.txt'
dd <- read.csv(file, sep = " ", header=F, stringsAsFactors = F)
file <- '/Users/voitel/TRAINING/UW_Data_Science/UW_Data_Science_450/Week2/monthly_avg_gas_usage_jan71_oct79.txt'
dd <- read.csv(file, sep = " ", header=F, stringsAsFactors = F)
dd
dat <- c(t(as.matrix(dd)))
acf(as.ts(dat))
acf(as.fs(dat))
acf(as.ts(dat))
dat
setwd('/Users/voitel/TRAINING/UW_Data_Science/UW_Data_Science_450/Week2')
file <- '/Users/voitel/TRAINING/UW_Data_Science/UW_Data_Science_450/Week2/monthly_avg_gas_usage_jan71_oct79.txt'
dd <- read.csv(file, sep = "", header=F, stringsAsFactors = F)
dd
dat <- c(t(as.matrix(dd)))
dat
acf(as.ts(dat))
acf(as.ts(na.omit(dat)))
setwd('/Users/voitel/TRAINING/UW_Data_Science/UW_Data_Science_450/Week2')
file <- '/Users/voitel/TRAINING/UW_Data_Science/UW_Data_Science_450/Week2/monthly_avg_gas_usage_jan71_oct79.txt'
file <- '/Users/voitel/TRAINING/UW_Data_Science/UW_Data_Science_450/Week2/monthly_avg_gas_usage_jan71_oct79.txt'
dd <- read.csv(file, sep = "", header=F, strip.white=T)
dd
dat <- c(t(as.matrix(dd)))
acf(as.ts(dat))
plot(dat, type='b')
mavg <- apply(dd, 2, mean, na.rm=T)
mavg
acf(as.ts(dat, na.rm=T))
acf(as.ts(na.omit(dat))
acf(as.ts(na.omit(dat)))
mavg <- apply(dd, 2, mean, na.rm=T)
plot(dat, type='b')
acf(as.ts(na.omit(dat)))
plot(dat, type='b')
monthplot(dat)
lm(data ~ c(1:108))
lm(dat ~ c(1:108))
arima(dat, order=c(1,0,0), seasonal=list(order=c(1,0,0), period=12))
fit <- arima(dat, order=c(1,0,0), seasonal=list(order=c(1,0,0), period=12))
aacf(fit$Residuals)
acf(fit$Residuals)
acf(fit$residuals)
acf(fit$residuals[c(1:106)])
fit <- arima(dat, order=c(1,0,0), seasonal=list(order=c(0,1,0), period=12))
acf(fit$residuals[c(1:106)])
pacf(fit$residuals)
pacf(fit$residuals[c(1:106)])
fit <- arima(dat, order=c(0,1,0), seasonal=list(order=c(0,1,0), period=12))
acf(fit$residuals[c(1:106)])
pacf(fit$residuals[c(1:106)])
fit <- arima(dat, order=c(0,1,0), seasonal=list(order=c(0,1,0), period=24))
acf(fit$residuals[c(1:106)])
pacf(fit$residuals[c(1:106)])
acf(as.ts(dat[c(1:106)]))
fit <- arima(dat, order=c(0,0,1), seasonal=list(order=c(1,0,0), period=6))
acf(fit$residuals[c(1:106)])
pacf(fit$residuals[c(1:106)])
fit <- arima(dat, order=c(0,0,1), seasonal=list(order=c(1,0,0), period=3))
acf(fit$residuals[c(1:106)])
pacf(fit$residuals[c(1:106)])
